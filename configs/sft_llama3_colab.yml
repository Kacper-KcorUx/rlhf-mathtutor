dataset_path: "/content/rlhf-mathtutor/data/processed/math_corpus.parquet"

base_model: "meta-llama/Llama-3-8b-instruct"
load_in_8bit: true
bnb_4bit_compute_dtype: "float16"
bnb_4bit_use_double_quant: true
bnb_4bit_use_cpu_offload: false
gradient_checkpointing: true

epochs: 1
per_device_batch_size: 2        # A100-40 GB lub L4-24 GB
gradient_accumulation: 16
lr: 2e-5
warmup_steps: 200
max_seq_len: 1024

output_dir: "runs/sft_llama3_baseline"
wandb_project: "rlhf-math-tutor"   # możesz usunąć, jeśli nie używasz WandB
